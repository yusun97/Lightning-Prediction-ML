{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import boto3\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "import xarray as xr\n",
    "import gzip\n",
    "import shutil\n",
    "import logging\n",
    "import pygrib\n",
    "import numpy as np\n",
    "import zarr\n",
    "import io\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress ECCODES warnings\n",
    "logging.getLogger('eccodes').setLevel(logging.ERROR)\n",
    "os.environ[\"ECCODES_WARNINGS\"] = \"0\"\n",
    "os.environ[\"ECCODES_LOG\"] = \"/dev/null\"\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_date_file_output_prefix = '/Users/jesse/Desktop/OpenSource/Lightning-Prediction-ML/zarr_0_1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. download the original s3 file and merge it into 1 single file for SHSR/Density/Prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_folder(bucket_name, folder_name):\n",
    "    s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED), region_name='us-east-1')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket_name, Prefix=folder_name)\n",
    "    \n",
    "    # Get all the files in the folder\n",
    "    file_keys = []\n",
    "    for page in pages:\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                file_keys.append(obj['Key'])\n",
    "    return file_keys\n",
    "\n",
    "def list_prob_files_in_folder(bucket_name, folder_name):\n",
    "    s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED), region_name='us-east-1')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket_name, Prefix=folder_name)\n",
    "    \n",
    "    # Get all the files in the folder\n",
    "    file_keys = []\n",
    "    for page in pages:\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                if obj['Key'][-13:-11] in(\"00\"):\n",
    "                    # print(obj['Key'])\n",
    "                    file_keys.append(obj['Key'])\n",
    "    return file_keys\n",
    "\n",
    "def make_negative_val_as_0(ds):\n",
    "    \n",
    "    ds['tmp'] = ds.unknown.fillna(value=0) \n",
    "    ds_negative = ds.where(ds['tmp'] >=0,0)\n",
    "    ds_positive = ds.where(ds['tmp']> 0)\n",
    "    ds = xr.merge([ds_negative, ds_positive])\n",
    "    data_dropped = ds.drop_vars(\"tmp\")\n",
    "    return data_dropped\n",
    "    \n",
    "def ct_prob_over_threshold_new(ds, ratio):\n",
    "\n",
    "\n",
    "    one_percent = ratio * ratio / 100\n",
    "    five_percent = ratio * ratio * 5 / 100\n",
    "    fifty_percent = ratio * ratio * 50 / 100\n",
    "    ds['prob_33_ct_gt_1'] = (ds['ds_coarsened_count_33'] > 1).astype(int)\n",
    "    ds['prob_33_ct_gt_1_per'] = (ds['ds_coarsened_count_33'] > one_percent).astype(int)\n",
    "    ds['prob_33_ct_gt_5_per'] =(ds['ds_coarsened_count_33'] > five_percent).astype(int)\n",
    "    ds['prob_33_ct_gt_50_per'] = (ds['ds_coarsened_count_33'] > fifty_percent).astype(int)\n",
    "\n",
    "    ds['prob_50_ct_gt_1'] = (ds['ds_coarsened_count_50'] > 1).astype(int)\n",
    "    ds['prob_50_ct_gt_1_per'] = (ds['ds_coarsened_count_50'] > one_percent).astype(int)\n",
    "    ds['prob_50_ct_gt_5_per'] = (ds['ds_coarsened_count_50'] > five_percent).astype(int)\n",
    "    ds['prob_50_ct_gt_50_per'] = (ds['ds_coarsened_count_50'] > fifty_percent).astype(int)\n",
    "\n",
    "\n",
    "def coarsen_001_025(ds_interp, expend_ratio):\n",
    "    # in our case the expend ration should be 25\n",
    "    coarsen_factor = {'latitude': expend_ratio, 'longitude': expend_ratio}\n",
    "\n",
    "    \n",
    "    # Sum\n",
    "    ds_coarsened_sum = ds_interp.coarsen(coarsen_factor, boundary='trim').sum()\n",
    "    new_var_names_sum = {var_name: f\"sum_{var_name}\" for var_name in ds_coarsened_sum.data_vars}\n",
    "    ds_coarsened_sum = ds_coarsened_sum.rename(new_var_names_sum)\n",
    "    \n",
    "    # Max\n",
    "    ds_coarsened_max = ds_interp.coarsen(coarsen_factor, boundary='trim').max()\n",
    "    new_var_names_max = {var_name: f\"max_{var_name}\" for var_name in ds_coarsened_max.data_vars}\n",
    "    ds_coarsened_max = ds_coarsened_max.rename(new_var_names_max)\n",
    "    \n",
    "    # Avg\n",
    "    ds_coarsened_avg = ds_interp.coarsen(coarsen_factor, boundary='trim').mean()\n",
    "    new_var_names_avg = {var_name: f\"avg_{var_name}\" for var_name in ds_coarsened_avg.data_vars}\n",
    "    ds_coarsened_avg = ds_coarsened_avg.rename(new_var_names_avg)\n",
    "    \n",
    "\n",
    "    # Merge sum and max datasets\n",
    "    ds_coarsened = xr.merge([ds_coarsened_sum, ds_coarsened_max, ds_coarsened_avg])\n",
    "    ds_coarsened_filled = ds_coarsened.fillna(0)\n",
    "\n",
    "    return ds_coarsened_filled\n",
    "\n",
    "def read_and_convert_s3_file(bucket_name, folder_name, output_zarr_path, append_flag, var_name):\n",
    "    s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED), region_name='us-east-1')\n",
    "    if var_name ==\"probility\":\n",
    "        file_keys= list_prob_files_in_folder(bucket_name, folder_name)\n",
    "    else:\n",
    "        file_keys = list_files_in_folder(bucket_name, folder_name)\n",
    "    \n",
    "    if not file_keys:\n",
    "        print(f\"No data available for {folder_name.split('/')[-2]}.\")\n",
    "        return\n",
    "    \n",
    "    for file_key in file_keys:\n",
    "        try:\n",
    "            compressed_content = s3_client.get_object(Bucket=bucket_name, Key=file_key)['Body'].read()\n",
    "            \n",
    "            with gzip.GzipFile(fileobj=io.BytesIO(compressed_content)) as gz:\n",
    "                grib_data = gz.read()\n",
    "            \n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix='.grib2') as temp_file:\n",
    "                temp_file.write(grib_data)\n",
    "                temp_file_path = temp_file.name\n",
    "            \n",
    "            ds = xr.open_dataset(temp_file_path, engine='cfgrib')\n",
    "            ds = ds.expand_dims('time')\n",
    "            ds = make_negative_val_as_0(ds)\n",
    "\n",
    "            \n",
    "            ds = ds.rename({\"unknown\": var_name})\n",
    "            # if \"prob\" in var_name:\n",
    "            #     ct_prob_over_threshold_new(ds, 10)\n",
    "            # else: \n",
    "            #     ds = coarsen_001_025(ds, 10)\n",
    " \n",
    "            if (not os.path.exists(output_zarr_path)) or (append_flag is False):\n",
    "                ds.to_zarr(output_zarr_path, mode='w')\n",
    "                append_flag = True\n",
    "            else:\n",
    "                ds.to_zarr(output_zarr_path, mode='a', append_dim='time')\n",
    "            \n",
    "            os.remove(temp_file_path)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_key}: {e}\")\n",
    "            continue\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    start_date = datetime.strptime(start_date, '%Y%m%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y%m%d')\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "def download_s3_file_per_day(bucket_name, data_type, var_name, start_date_str, end_date_str, save_var_name):\n",
    "\n",
    "    for single_date in daterange(start_date_str, end_date_str):\n",
    "        date_str = single_date.strftime('%Y%m%d')\n",
    "        output_zarr_path = f\"/Users/jesse/Desktop/OpenSource/Lightning-Prediction-ML/zarr_0_1/{save_var_name}/{save_var_name}_{date_str}.zarr\"\n",
    "        folder_name = f'CONUS/{data_type}/{date_str}/'\n",
    "        read_and_convert_s3_file(bucket_name, folder_name, output_zarr_path, False, var_name)\n",
    "        print(start_date_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_download_single_day_file():\n",
    "    bucket_name = 'noaa-mrms-pds'\n",
    "    data_type = 'SeamlessHSR_00.00'\n",
    "    var_name = \"shsr\"\n",
    "    start_date_str = '20230801'\n",
    "    end_date_str = '20230807'\n",
    "    save_var_name = 'shsr002'\n",
    "    \n",
    "    download_s3_file_per_day(bucket_name, data_type, var_name, start_date_str, end_date_str,save_var_name)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_download_single_day_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Merge Single day file into 1 merged Zarr file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toHour(ds):\n",
    "    ds = ds.sortby('time')\n",
    "    ds = ds.sel(time=~ds.get_index('time').duplicated())\n",
    "    variables_to_resample = ds.data_vars\n",
    "    # resample shsr\n",
    "    resampled = {}\n",
    "    for var in variables_to_resample:\n",
    "        resampled[f'{var}_1hr_max'] = ds[var].resample(time='1H').max()\n",
    "        resampled[f'{var}_1hr_min'] = ds[var].resample(time='1H').min()\n",
    "        resampled[f'{var}_1hr_std'] = ds[var].resample(time='1H').std()\n",
    "        resampled[f'{var}_1hr_sum'] = ds[var].resample(time='1H').sum()\n",
    "    # merge it back to 1 zarr\n",
    "    ds_resampled = xr.Dataset(resampled)\n",
    "    return ds_resampled\n",
    "\n",
    "def daterange(start_date_str, end_date_str):\n",
    "    start_date = datetime.strptime(start_date_str, '%Y%m%d')\n",
    "    end_date = datetime.strptime(end_date_str, '%Y%m%d')\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        yield start_date + timedelta(n)\n",
    "        \n",
    "\n",
    "def merge_to_1_zarr(var_name, start_date_str,end_date_str, append_flag,output_file_path):\n",
    "    \n",
    "    for single_date in daterange(start_date_str, end_date_str):\n",
    "        date_str = single_date.strftime('%Y%m%d')\n",
    "        file_path = f\"/Users/jesse/Desktop/OpenSource/Lightning-Prediction-ML/zarr_0_1/{var_name}/{var_name}_{date_str}.zarr\"\n",
    "        \n",
    "        try:\n",
    "            ds = xr.open_zarr(file_path)\n",
    "            ds = ds.drop_vars('valid_time')\n",
    "            ds = toHour(ds)\n",
    "            \n",
    "            if (not os.path.exists(output_file_path)) or (append_flag is False):\n",
    "                ds.to_zarr(output_file_path, mode='w')\n",
    "                append_flag = True\n",
    "            else:\n",
    "                ds.to_zarr(output_file_path, mode='a', append_dim='time')\n",
    "            print(date_str, \"completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {date_str}: {e}\")\n",
    "            \n",
    "\n",
    "def select_time(ds, start_date, end_date):\n",
    "    time_range = slice(start_date, end_date)\n",
    "    ds = ds.sel(time = time_range)\n",
    "    return ds\n",
    "\n",
    "def round_up_lat_lon(ds, ratio):\n",
    "    new_longitude = np.around(ds[\"longitude\"] / ratio) * ratio\n",
    "    new_latitude = np.around(ds[\"latitude\"] / ratio) * ratio\n",
    "\n",
    "    ds_interp = ds.interp(\n",
    "        latitude=new_latitude, longitude=new_longitude, method=\"nearest\"\n",
    "    )\n",
    "\n",
    "    ds_interp[\"longitude\"] = np.around(ds_interp[\"longitude\"] // ratio) * ratio\n",
    "    ds_interp[\"latitude\"] = np.around(ds_interp[\"latitude\"] // ratio) * ratio\n",
    "    return ds_interp\n",
    "\n",
    "def merge_ds_to_df_all(ds_list, start_date,end_date,ratio):\n",
    "    \n",
    "    for i  in range(len(ds_list)):\n",
    "    \n",
    "        # ds_list[i] = select_time(ds_list[i],start_date,end_date)\n",
    "        # ds_list[i] = round_up_lat_lon(ds_list[i], ratio)\n",
    "        # print(ds_list[i])\n",
    "        # if 'sum_shsr_1hr_sum' in ds_list[i].data_vars:\n",
    "        #     ds_list[i] = ds_list[i].where(ds_list[i]['sum_shsr_1hr_sum'] > 0, drop=True)\n",
    "        ds_list[i] = ds_list[i].to_dataframe()\n",
    "        ds_list[i] = ds_list[i].set_index(['time','latitude','longitude'])\n",
    "    \n",
    "    _path = \"/Users/jesse/Desktop/OpenSource/Lightning-Prediction-ML/zarr_0_1/merged/\"\n",
    "    if not os.path.exists(_path):\n",
    "        os.makedirs(_path)\n",
    "   \n",
    "    df_merged = ds_list[0].merge(ds_list[1], on=['time','latitude','longitude']).merge(ds_list[2], on=['time','latitude','longitude'])\n",
    "    # df_filtered = df_merged[df_merged['sum_shsr_1hr_sum'] > 0]\n",
    "    # df_filtered = df_merged\n",
    "    df_merged.to_parquet(os.path.join(_path, '2022Train_w_probHr_002.parquet'))\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_merge_into_one_zarr_for_3_categories():\n",
    "    var_name = \"shsr002\"\n",
    "    start_date_str = '20220801'\n",
    "    end_date_str = '20220807'\n",
    "    # output_file_path = f'/Users/jesse/Desktop/OpenSource/Lightning-Prediction-ML/zarr_0_1/merged/20220801_20220807_{var_name}.zarr'\n",
    "    # merge_to_1_zarr(var_name,start_date_str,end_date_str,False,output_file_path)\n",
    "    \n",
    "    t_ds_density = xr.open_dataset(\"/Users/jesse/Desktop/OpenSource/Lightning-Prediction-ML/zarr_0_1/merged/20220801_20220807_density002.zarr\")\n",
    "    t_ds_shsr   = xr.open_dataset(\"/Users/jesse/Desktop/OpenSource/Lightning-Prediction-ML/zarr_0_1/merged/20220801_20220807_shsr002.zarr\")\n",
    "    t_ds_prob = xr.open_dataset(\"/Users/jesse/Desktop/OpenSource/Lightning-Prediction-ML/zarr_0_1/merged/20220801_20220807_prob002.zarr\")\n",
    "  \n",
    " \n",
    "    ds_list = [t_ds_density,t_ds_shsr,t_ds_prob]\n",
    "    start_date ='2023-08-01'\n",
    "    end_date = '2023-08-07'\n",
    "    ratio = 1\n",
    "    training_merged_df = merge_ds_to_df_all(ds_list, start_date,end_date,ratio)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = pd.read_parquet(\"/Users/jesse/Desktop/OpenSource/Lightning-Prediction-ML/zarr_0_1/merged/2022Train_w_probHr_002.parquet\")\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/xarray/backends/plugins.py:159: RuntimeWarning: 'scipy' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n",
      "/opt/homebrew/lib/python3.12/site-packages/xarray/backends/plugins.py:159: RuntimeWarning: 'scipy' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n",
      "/opt/homebrew/lib/python3.12/site-packages/xarray/backends/plugins.py:159: RuntimeWarning: 'scipy' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "second_merge_into_one_zarr_for_3_categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add Static Var + SHSR In Pre/Post Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_geographic_local_time(longitude, utc_datetime):\n",
    "    timezone_offset = timedelta(hours=longitude / 15)\n",
    "    local_time = utc_datetime + timezone_offset\n",
    "    return local_time\n",
    "\n",
    "def floor_to_nearest_hour(dt):\n",
    "    return dt.replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "def get_season(latitude, local_time):\n",
    "    month = local_time.month\n",
    "    # Northern Hemisphere \n",
    "    if latitude >= 0:  \n",
    "        if 3 <= month <= 5:\n",
    "            return 'Spring'\n",
    "        elif 6 <= month <= 8:\n",
    "            return 'Summer'\n",
    "        elif 9 <= month <= 11:\n",
    "            return 'Autumn'\n",
    "        else:\n",
    "            return 'Winter'\n",
    "    else:  # Southern Hemisphere\n",
    "        if 3 <= month <= 5:\n",
    "            return 'Autumn'\n",
    "        elif 6 <= month <= 8:\n",
    "            return 'Winter'\n",
    "        elif 9 <= month <= 11:\n",
    "            return 'Spring'\n",
    "        else:\n",
    "            return 'Summer'\n",
    "        \n",
    "def add_static_var(df):\n",
    "\n",
    "    local_times = []\n",
    "    local_seasons = []\n",
    "    for index, _ in df.iterrows():\n",
    "        time, latitude, longitude = index\n",
    "        if longitude > 180: longitude -= 360\n",
    "        if latitude >90: latitude-=180\n",
    "        local_time = calculate_geographic_local_time(longitude, time)\n",
    "        local_season = get_season(latitude,time)\n",
    "        local_times.append(local_time)\n",
    "        local_seasons.append(local_season)\n",
    "\n",
    "    # 1. get local time\n",
    "    df['local_time'] = local_times \n",
    "    df['local_time'] = df['local_time'].apply(floor_to_nearest_hour)\n",
    "    # 2. add day of year\n",
    "    df['day_of_year'] = df['local_time'].dt.dayofyear\n",
    "    # 3. add time of day\n",
    "    df['hour_of_day'] = df['local_time'].apply(lambda x: x.hour)\n",
    "    # 4. add local season\n",
    "    df['season'] = local_seasons\n",
    "    df['isSummer'] = df['season'] == 'Summer'\n",
    "    df['isSpring'] = df['season'] == 'Spring'\n",
    "    df['isWinter'] = df['season'] == 'Winter'\n",
    "    df['isAutumn'] = df['season'] == 'Autumn'\n",
    "    df.sort_values(by='local_time', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_shsr1_lightning1(df):\n",
    "    df_lightning_positive = df[df['sum_density_1hr_sum'] > 0]\n",
    "    df_return = df_lightning_positive[df_lightning_positive[\"avg_shsr_1hr_sum\"]>0]\n",
    "    return df_return\n",
    "\n",
    "\n",
    "def get_shsr1_lightning0(df):\n",
    "    df_shsr_positive = df[df['avg_shsr_1hr_sum'] > 0]\n",
    "    df_shsr_positive['sum_density_1hr_sum'] = df_shsr_positive['sum_density_1hr_sum'].fillna(value=0) \n",
    "    df_return = df_shsr_positive.dropna(subset=['sum_density_1hr_sum'])\n",
    "    return df_return    \n",
    "\n",
    "\n",
    "def get_shsr1_lightning1_test(df):\n",
    "    df_lightning_positive = df[df['sum_density_1hr_sum'] > 0]\n",
    "    df_return = df_lightning_positive[df_lightning_positive[\"avg_shsr_1hr_sum\"]>0]\n",
    "    return df_return\n",
    "\n",
    "\n",
    "def get_shsr1_lightning0_test(df):\n",
    "    df_shsr_positive = df[df['avg_shsr_1hr_sum'] > 0]\n",
    "    df_shsr_positive['sum_density_1hr_sum'] = df_shsr_positive['sum_density_1hr_sum'].fillna(value=0) \n",
    "    df_return = df_shsr_positive.dropna(subset=['sum_density_1hr_sum'])\n",
    "    return df_return    \n",
    "\n",
    "\n",
    "def add_shsr_ct_vars(df):\n",
    "    df['avg_shsr_1hr_sum_gt_30'] = df['avg_shsr_1hr_sum'] > 30\n",
    "    df['avg_shsr_1hr_sum_gt_40'] = df['avg_shsr_1hr_sum'] > 40\n",
    "    df['avg_shsr_1hr_sum_gt_50'] = df['avg_shsr_1hr_sum'] > 50\n",
    "    \n",
    "    return df\n",
    "\n",
    "shsr_var = [\"sum_shsr_1hr_sum\",\"avg_shsr_1hr_max\",\"avg_shsr_1hr_sum\",\n",
    "            \"max_shsr_1hr_max\",\"max_shsr_1hr_std\",\"max_shsr_1hr_sum\",\"sum_shsr_1hr_max\",\n",
    "            \"sum_shsr_1hr_min\",\"sum_shsr_1hr_std\",\n",
    "            ]\n",
    "\n",
    "def add_SHSR_feature(df):\n",
    "    vars = shsr_var\n",
    "    for var in vars:\n",
    "        # prev 4 hr\n",
    "        df[f'{var}_prev_4hr_sum'] = df[var].shift(1).rolling(window=4, min_periods=1).sum()\n",
    "        # after 2hr \n",
    "        df[f'{var}_after_2hr_sum'] = df[var].shift(-1)+df[var].shift(-2)\n",
    "        # after 4hr \n",
    "        df[f'{var}_after_4hr_sum'] = df[var].shift(-1)+df[var].shift(-2)+df[var].shift(-3)+df[var].shift(-4)\n",
    "\n",
    "    # weather_shsr_var = ['tcc','d2m','t2m','ws'] + total_variables\n",
    "    tmp_Vars = [\"sum_shsr_1hr_sum\",\"avg_shsr_1hr_max\",\"avg_shsr_1hr_sum\",\n",
    "                \"max_shsr_1hr_max\",\"max_shsr_1hr_std\",\"max_shsr_1hr_sum\",\"sum_shsr_1hr_max\",\n",
    "                \"sum_shsr_1hr_min\",\"sum_shsr_1hr_std\",\"avg_shsr_1hr_sum_gt_30\",\"avg_shsr_1hr_sum_gt_40\",\n",
    "                \"avg_shsr_1hr_sum_gt_50\",\"sum_density_1hr_sum\"]\n",
    "    for var in tmp_Vars:\n",
    "        # 创建四小时前到一小时前的列\n",
    "        for i in range(1, 5):\n",
    "            df[f'{var}_at-{i}hr'] = df[var].shift(i)\n",
    "\n",
    "        # 创建一小时后到三小时后的列\n",
    "        if var == \"sum_lightning_density_1hr_sum\":\n",
    "            continue\n",
    "        for i in range(1, 4):\n",
    "            df[f'{var}_at+{i}hr'] = df[var].shift(-i)\n",
    "\n",
    "    df1 = df.fillna(0)\n",
    "    return df1\n",
    "\n",
    "def change_dtype_for_parquet(df_train):\n",
    "    for column in df_train.columns:\n",
    "        if df_train[column].dtype == 'object':\n",
    "            if column == 'local_time':\n",
    "                try:\n",
    "                    # 将 'local_time' 列转换为 datetime64 类型\n",
    "                    df_train[column] = pd.to_datetime(df_train[column], errors='coerce')\n",
    "                    print(f\"Converted column '{column}' to datetime.\")\n",
    "                except ValueError:\n",
    "                    print(f\"Column '{column}' cannot be converted to datetime and will be skipped.\")\n",
    "            elif column == 'season':\n",
    "                try:\n",
    "                    # 将 'season' 列转换为 category 类型\n",
    "                    df_train[column] = df_train[column].astype('string')\n",
    "                except ValueError:\n",
    "                    print(f\"Column '{column}' cannot be converted to category and will be skipped.\")\n",
    "            else:\n",
    "                try:\n",
    "                    # 尝试将其他列转换为 bool 类型\n",
    "                    df_train[column] = df_train[column].astype('bool')\n",
    "                except ValueError:\n",
    "                    # 如果转换失败，则保留原类型并打印警告\n",
    "                    print(f\"Column '{column}' cannot be converted to bool and will be skipped.\")\n",
    "    return df_train\n",
    "\n",
    "\n",
    "def third_generate_the_training_file_with_static_and_shsr_features():\n",
    "\n",
    "    ds_train = pd.read_parquet(\"/Users/jesse/Desktop/OpenSource/Lightning-Prediction-ML/zarr_0_1/merged/2023Test_w_probHr_per1.parquet\")\n",
    "    df_train = ds_train.reset_index()\n",
    "    df1 = get_shsr1_lightning1(df_train)\n",
    "    df2 =  get_shsr1_lightning0(df_train)\n",
    "    df1 = df1.reset_index(drop=True)\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "    df_train_merged = pd.concat([df1, df2], axis=0,ignore_index=True)\n",
    "    df_train_merged = df_train_merged.set_index([\"time\", \"latitude\", \"longitude\"])\n",
    "    \n",
    "    df_static = add_static_var(df_train_merged)\n",
    "    df_static = df_static.dropna(subset=['avg_shsr_1hr_sum'])\n",
    "    df_test_merged= add_shsr_ct_vars(df_static)\n",
    "    df_static = add_SHSR_feature(df_static)\n",
    "    print(df_test_merged.columns)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    parquet_output_path = \"/Users/jesse/Desktop/OpenSource/Lightning-Prediction-ML/zarr_0_1/merged/test_merged_202409_prevpost_per1.parquet\"\n",
    "    df_test_merged.to_parquet(parquet_output_path, index=True)\n",
    "\n",
    "    print(f\"Data saved as Parquet at {parquet_output_path}\")\n",
    "    \n",
    "    \n",
    "    return df_test_merged\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at+{i}hr'] = df[var].shift(-i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at-{i}hr'] = df[var].shift(i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at-{i}hr'] = df[var].shift(i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at-{i}hr'] = df[var].shift(i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at-{i}hr'] = df[var].shift(i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at+{i}hr'] = df[var].shift(-i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at+{i}hr'] = df[var].shift(-i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at+{i}hr'] = df[var].shift(-i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at-{i}hr'] = df[var].shift(i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at-{i}hr'] = df[var].shift(i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at-{i}hr'] = df[var].shift(i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at-{i}hr'] = df[var].shift(i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at+{i}hr'] = df[var].shift(-i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at+{i}hr'] = df[var].shift(-i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at+{i}hr'] = df[var].shift(-i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at-{i}hr'] = df[var].shift(i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at-{i}hr'] = df[var].shift(i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at-{i}hr'] = df[var].shift(i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at-{i}hr'] = df[var].shift(i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at+{i}hr'] = df[var].shift(-i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at+{i}hr'] = df[var].shift(-i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at+{i}hr'] = df[var].shift(-i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at-{i}hr'] = df[var].shift(i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at-{i}hr'] = df[var].shift(i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at-{i}hr'] = df[var].shift(i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at-{i}hr'] = df[var].shift(i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at+{i}hr'] = df[var].shift(-i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at+{i}hr'] = df[var].shift(-i)\n",
      "/var/folders/zf/my42n5_x09g47p6r08bz8_1w0000gn/T/ipykernel_21822/306381155.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{var}_at+{i}hr'] = df[var].shift(-i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['avg_density_1hr_max', 'avg_density_1hr_min', 'avg_density_1hr_std',\n",
      "       'avg_density_1hr_sum', 'heightAboveSea_x', 'max_density_1hr_max',\n",
      "       'max_density_1hr_min', 'max_density_1hr_std', 'max_density_1hr_sum',\n",
      "       'step_x',\n",
      "       ...\n",
      "       'avg_shsr_1hr_sum_gt_50_at+1hr', 'avg_shsr_1hr_sum_gt_50_at+2hr',\n",
      "       'avg_shsr_1hr_sum_gt_50_at+3hr', 'sum_density_1hr_sum_at-1hr',\n",
      "       'sum_density_1hr_sum_at-2hr', 'sum_density_1hr_sum_at-3hr',\n",
      "       'sum_density_1hr_sum_at-4hr', 'sum_density_1hr_sum_at+1hr',\n",
      "       'sum_density_1hr_sum_at+2hr', 'sum_density_1hr_sum_at+3hr'],\n",
      "      dtype='object', length=170)\n",
      "Data saved as Parquet at /Users/jesse/Desktop/OpenSource/Lightning-Prediction-ML/zarr_0_1/merged/test_merged_202409_prevpost_per1.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>avg_density_1hr_max</th>\n",
       "      <th>avg_density_1hr_min</th>\n",
       "      <th>avg_density_1hr_std</th>\n",
       "      <th>avg_density_1hr_sum</th>\n",
       "      <th>heightAboveSea_x</th>\n",
       "      <th>max_density_1hr_max</th>\n",
       "      <th>max_density_1hr_min</th>\n",
       "      <th>max_density_1hr_std</th>\n",
       "      <th>max_density_1hr_sum</th>\n",
       "      <th>step_x</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_shsr_1hr_sum_gt_50_at+1hr</th>\n",
       "      <th>avg_shsr_1hr_sum_gt_50_at+2hr</th>\n",
       "      <th>avg_shsr_1hr_sum_gt_50_at+3hr</th>\n",
       "      <th>sum_density_1hr_sum_at-1hr</th>\n",
       "      <th>sum_density_1hr_sum_at-2hr</th>\n",
       "      <th>sum_density_1hr_sum_at-3hr</th>\n",
       "      <th>sum_density_1hr_sum_at-4hr</th>\n",
       "      <th>sum_density_1hr_sum_at+1hr</th>\n",
       "      <th>sum_density_1hr_sum_at+2hr</th>\n",
       "      <th>sum_density_1hr_sum_at+3hr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2023-07-09 00:00:00</th>\n",
       "      <th>50.7</th>\n",
       "      <th>236.3</th>\n",
       "      <td>0.01276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>0.35728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.636581</td>\n",
       "      <td>35.728001</td>\n",
       "      <td>0 days</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">43.7</th>\n",
       "      <th>237.9</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0 days</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>35.728001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237.7</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0 days</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.728001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.379999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237.6</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0 days</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.728001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.379999</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43.8</th>\n",
       "      <th>239.9</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0 days</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.728001</td>\n",
       "      <td>22.379999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2023-07-15 16:00:00</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">41.2</th>\n",
       "      <th>288.4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0 days</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288.5</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0 days</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288.6</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0 days</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46.2</th>\n",
       "      <th>291.5</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0 days</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45.6</th>\n",
       "      <th>294.5</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0 days</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1849129 rows × 170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        avg_density_1hr_max  \\\n",
       "time                latitude longitude                        \n",
       "2023-07-09 00:00:00 50.7     236.3                  0.01276   \n",
       "                    43.7     237.9                  0.00000   \n",
       "                             237.7                  0.00000   \n",
       "                             237.6                  0.00000   \n",
       "                    43.8     239.9                  0.00000   \n",
       "...                                                     ...   \n",
       "2023-07-15 16:00:00 41.2     288.4                  0.00000   \n",
       "                             288.5                  0.00000   \n",
       "                             288.6                  0.00000   \n",
       "                    46.2     291.5                  0.00000   \n",
       "                    45.6     294.5                  0.00000   \n",
       "\n",
       "                                        avg_density_1hr_min  \\\n",
       "time                latitude longitude                        \n",
       "2023-07-09 00:00:00 50.7     236.3                      0.0   \n",
       "                    43.7     237.9                      0.0   \n",
       "                             237.7                      0.0   \n",
       "                             237.6                      0.0   \n",
       "                    43.8     239.9                      0.0   \n",
       "...                                                     ...   \n",
       "2023-07-15 16:00:00 41.2     288.4                      0.0   \n",
       "                             288.5                      0.0   \n",
       "                             288.6                      0.0   \n",
       "                    46.2     291.5                      0.0   \n",
       "                    45.6     294.5                      0.0   \n",
       "\n",
       "                                        avg_density_1hr_std  \\\n",
       "time                latitude longitude                        \n",
       "2023-07-09 00:00:00 50.7     236.3                 0.006366   \n",
       "                    43.7     237.9                 0.000000   \n",
       "                             237.7                 0.000000   \n",
       "                             237.6                 0.000000   \n",
       "                    43.8     239.9                 0.000000   \n",
       "...                                                     ...   \n",
       "2023-07-15 16:00:00 41.2     288.4                 0.000000   \n",
       "                             288.5                 0.000000   \n",
       "                             288.6                 0.000000   \n",
       "                    46.2     291.5                 0.000000   \n",
       "                    45.6     294.5                 0.000000   \n",
       "\n",
       "                                        avg_density_1hr_sum  heightAboveSea_x  \\\n",
       "time                latitude longitude                                          \n",
       "2023-07-09 00:00:00 50.7     236.3                  0.35728               0.0   \n",
       "                    43.7     237.9                  0.00000               0.0   \n",
       "                             237.7                  0.00000               0.0   \n",
       "                             237.6                  0.00000               0.0   \n",
       "                    43.8     239.9                  0.00000               0.0   \n",
       "...                                                     ...               ...   \n",
       "2023-07-15 16:00:00 41.2     288.4                  0.00000               0.0   \n",
       "                             288.5                  0.00000               0.0   \n",
       "                             288.6                  0.00000               0.0   \n",
       "                    46.2     291.5                  0.00000               0.0   \n",
       "                    45.6     294.5                  0.00000               0.0   \n",
       "\n",
       "                                        max_density_1hr_max  \\\n",
       "time                latitude longitude                        \n",
       "2023-07-09 00:00:00 50.7     236.3                    1.276   \n",
       "                    43.7     237.9                    0.000   \n",
       "                             237.7                    0.000   \n",
       "                             237.6                    0.000   \n",
       "                    43.8     239.9                    0.000   \n",
       "...                                                     ...   \n",
       "2023-07-15 16:00:00 41.2     288.4                    0.000   \n",
       "                             288.5                    0.000   \n",
       "                             288.6                    0.000   \n",
       "                    46.2     291.5                    0.000   \n",
       "                    45.6     294.5                    0.000   \n",
       "\n",
       "                                        max_density_1hr_min  \\\n",
       "time                latitude longitude                        \n",
       "2023-07-09 00:00:00 50.7     236.3                      0.0   \n",
       "                    43.7     237.9                      0.0   \n",
       "                             237.7                      0.0   \n",
       "                             237.6                      0.0   \n",
       "                    43.8     239.9                      0.0   \n",
       "...                                                     ...   \n",
       "2023-07-15 16:00:00 41.2     288.4                      0.0   \n",
       "                             288.5                      0.0   \n",
       "                             288.6                      0.0   \n",
       "                    46.2     291.5                      0.0   \n",
       "                    45.6     294.5                      0.0   \n",
       "\n",
       "                                        max_density_1hr_std  \\\n",
       "time                latitude longitude                        \n",
       "2023-07-09 00:00:00 50.7     236.3                 0.636581   \n",
       "                    43.7     237.9                 0.000000   \n",
       "                             237.7                 0.000000   \n",
       "                             237.6                 0.000000   \n",
       "                    43.8     239.9                 0.000000   \n",
       "...                                                     ...   \n",
       "2023-07-15 16:00:00 41.2     288.4                 0.000000   \n",
       "                             288.5                 0.000000   \n",
       "                             288.6                 0.000000   \n",
       "                    46.2     291.5                 0.000000   \n",
       "                    45.6     294.5                 0.000000   \n",
       "\n",
       "                                        max_density_1hr_sum step_x  ...  \\\n",
       "time                latitude longitude                              ...   \n",
       "2023-07-09 00:00:00 50.7     236.3                35.728001 0 days  ...   \n",
       "                    43.7     237.9                 0.000000 0 days  ...   \n",
       "                             237.7                 0.000000 0 days  ...   \n",
       "                             237.6                 0.000000 0 days  ...   \n",
       "                    43.8     239.9                 0.000000 0 days  ...   \n",
       "...                                                     ...    ...  ...   \n",
       "2023-07-15 16:00:00 41.2     288.4                 0.000000 0 days  ...   \n",
       "                             288.5                 0.000000 0 days  ...   \n",
       "                             288.6                 0.000000 0 days  ...   \n",
       "                    46.2     291.5                 0.000000 0 days  ...   \n",
       "                    45.6     294.5                 0.000000 0 days  ...   \n",
       "\n",
       "                                        avg_shsr_1hr_sum_gt_50_at+1hr  \\\n",
       "time                latitude longitude                                  \n",
       "2023-07-09 00:00:00 50.7     236.3                              False   \n",
       "                    43.7     237.9                              False   \n",
       "                             237.7                              False   \n",
       "                             237.6                               True   \n",
       "                    43.8     239.9                               True   \n",
       "...                                                               ...   \n",
       "2023-07-15 16:00:00 41.2     288.4                              False   \n",
       "                             288.5                              False   \n",
       "                             288.6                               True   \n",
       "                    46.2     291.5                              False   \n",
       "                    45.6     294.5                                NaN   \n",
       "\n",
       "                                        avg_shsr_1hr_sum_gt_50_at+2hr  \\\n",
       "time                latitude longitude                                  \n",
       "2023-07-09 00:00:00 50.7     236.3                              False   \n",
       "                    43.7     237.9                              False   \n",
       "                             237.7                               True   \n",
       "                             237.6                               True   \n",
       "                    43.8     239.9                               True   \n",
       "...                                                               ...   \n",
       "2023-07-15 16:00:00 41.2     288.4                              False   \n",
       "                             288.5                               True   \n",
       "                             288.6                              False   \n",
       "                    46.2     291.5                                NaN   \n",
       "                    45.6     294.5                                NaN   \n",
       "\n",
       "                                        avg_shsr_1hr_sum_gt_50_at+3hr  \\\n",
       "time                latitude longitude                                  \n",
       "2023-07-09 00:00:00 50.7     236.3                              False   \n",
       "                    43.7     237.9                               True   \n",
       "                             237.7                               True   \n",
       "                             237.6                               True   \n",
       "                    43.8     239.9                               True   \n",
       "...                                                               ...   \n",
       "2023-07-15 16:00:00 41.2     288.4                               True   \n",
       "                             288.5                              False   \n",
       "                             288.6                                NaN   \n",
       "                    46.2     291.5                                NaN   \n",
       "                    45.6     294.5                                NaN   \n",
       "\n",
       "                                        sum_density_1hr_sum_at-1hr  \\\n",
       "time                latitude longitude                               \n",
       "2023-07-09 00:00:00 50.7     236.3                             NaN   \n",
       "                    43.7     237.9                       35.728001   \n",
       "                             237.7                        0.000000   \n",
       "                             237.6                        0.000000   \n",
       "                    43.8     239.9                        0.000000   \n",
       "...                                                            ...   \n",
       "2023-07-15 16:00:00 41.2     288.4                        0.000000   \n",
       "                             288.5                        0.000000   \n",
       "                             288.6                        0.000000   \n",
       "                    46.2     291.5                        0.000000   \n",
       "                    45.6     294.5                        0.000000   \n",
       "\n",
       "                                        sum_density_1hr_sum_at-2hr  \\\n",
       "time                latitude longitude                               \n",
       "2023-07-09 00:00:00 50.7     236.3                             NaN   \n",
       "                    43.7     237.9                             NaN   \n",
       "                             237.7                       35.728001   \n",
       "                             237.6                        0.000000   \n",
       "                    43.8     239.9                        0.000000   \n",
       "...                                                            ...   \n",
       "2023-07-15 16:00:00 41.2     288.4                        0.000000   \n",
       "                             288.5                        0.000000   \n",
       "                             288.6                        0.000000   \n",
       "                    46.2     291.5                        0.000000   \n",
       "                    45.6     294.5                        0.000000   \n",
       "\n",
       "                                        sum_density_1hr_sum_at-3hr  \\\n",
       "time                latitude longitude                               \n",
       "2023-07-09 00:00:00 50.7     236.3                             NaN   \n",
       "                    43.7     237.9                             NaN   \n",
       "                             237.7                             NaN   \n",
       "                             237.6                       35.728001   \n",
       "                    43.8     239.9                        0.000000   \n",
       "...                                                            ...   \n",
       "2023-07-15 16:00:00 41.2     288.4                        0.000000   \n",
       "                             288.5                        0.000000   \n",
       "                             288.6                        0.000000   \n",
       "                    46.2     291.5                        0.000000   \n",
       "                    45.6     294.5                        0.000000   \n",
       "\n",
       "                                        sum_density_1hr_sum_at-4hr  \\\n",
       "time                latitude longitude                               \n",
       "2023-07-09 00:00:00 50.7     236.3                             NaN   \n",
       "                    43.7     237.9                             NaN   \n",
       "                             237.7                             NaN   \n",
       "                             237.6                             NaN   \n",
       "                    43.8     239.9                       35.728001   \n",
       "...                                                            ...   \n",
       "2023-07-15 16:00:00 41.2     288.4                        0.000000   \n",
       "                             288.5                        0.000000   \n",
       "                             288.6                        0.000000   \n",
       "                    46.2     291.5                        0.000000   \n",
       "                    45.6     294.5                        0.000000   \n",
       "\n",
       "                                        sum_density_1hr_sum_at+1hr  \\\n",
       "time                latitude longitude                               \n",
       "2023-07-09 00:00:00 50.7     236.3                        0.000000   \n",
       "                    43.7     237.9                        0.000000   \n",
       "                             237.7                        0.000000   \n",
       "                             237.6                        0.000000   \n",
       "                    43.8     239.9                       22.379999   \n",
       "...                                                            ...   \n",
       "2023-07-15 16:00:00 41.2     288.4                        0.000000   \n",
       "                             288.5                        0.000000   \n",
       "                             288.6                        0.000000   \n",
       "                    46.2     291.5                        0.000000   \n",
       "                    45.6     294.5                             NaN   \n",
       "\n",
       "                                        sum_density_1hr_sum_at+2hr  \\\n",
       "time                latitude longitude                               \n",
       "2023-07-09 00:00:00 50.7     236.3                        0.000000   \n",
       "                    43.7     237.9                        0.000000   \n",
       "                             237.7                        0.000000   \n",
       "                             237.6                       22.379999   \n",
       "                    43.8     239.9                        0.000000   \n",
       "...                                                            ...   \n",
       "2023-07-15 16:00:00 41.2     288.4                        0.000000   \n",
       "                             288.5                        0.000000   \n",
       "                             288.6                        0.000000   \n",
       "                    46.2     291.5                             NaN   \n",
       "                    45.6     294.5                             NaN   \n",
       "\n",
       "                                        sum_density_1hr_sum_at+3hr  \n",
       "time                latitude longitude                              \n",
       "2023-07-09 00:00:00 50.7     236.3                        0.000000  \n",
       "                    43.7     237.9                        0.000000  \n",
       "                             237.7                       22.379999  \n",
       "                             237.6                        0.000000  \n",
       "                    43.8     239.9                        0.000000  \n",
       "...                                                            ...  \n",
       "2023-07-15 16:00:00 41.2     288.4                        0.000000  \n",
       "                             288.5                        0.000000  \n",
       "                             288.6                             NaN  \n",
       "                    46.2     291.5                             NaN  \n",
       "                    45.6     294.5                             NaN  \n",
       "\n",
       "[1849129 rows x 170 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_generate_the_training_file_with_static_and_shsr_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
